# 线代基础

## 矩阵相乘

![image-20241201235154772](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201235154772.png)

![image-20241201230406176](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201230406176.png)

## 特征向量

![image-20241201233413348](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201233413348.png)

# 2-1

## 箱形图的计算

![image-20241201223554055](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201223554055.png)

这个四分之一是**n+1**

![image-20241201223525459](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201223525459.png)

## 数据规范化计算

![image-20241201223935014](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201223935014.png)

## PCA主成成分分析

线代的计算

![image-20241201225917170](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201225917170.png)

降维的体现

![image-20241201225733763](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201225733763.png)

![image-20241201225546916](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201225546916.png)

![image-20241201224647587](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201224647587.png)

### 计算步骤

![image-20241201225229981](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201225229981.png)

### 计算实例

![image-20241201230816167](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201230816167.png)

![image-20241201234444191](C:%5CUsers%5C%E8%B0%A2%E9%9A%86%E6%9D%B0%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241201234444191.png)

### 缺点

- 对于高阶线性相关不适用
- 由于PCA是一种无参技术，没办法个性化的优化
- 如果非正交方向存在几个方差较大的方向，pca的效果会有折扣



### 应用

- 人脸识别
- 过程检测
- 法官评分

# 2-2 采样

几种处理手段

![image-20241126134939959](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126134939959.png)

我的理解是因为小样本比例小所以影响程度应该更大

![image-20241126135518003](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126135518003.png)

![image-20241126135252154](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126135252154.png)

数据采样

![image-20241126135617483](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126135617483.png)

- 简单上采样：复制小的样本但是会导致过拟合

- 简单下采样：删除太多的样本，可能会导致丢失重要信息

  **smote**

  `主要公式`

  ![image-20241126141129030](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126141129030.png)

![image-20241126140124656](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126140124656.png)

![image-20241126140307808](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126140307808.png)

# 回归

基本概念

![image-20241126142009469](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126142009469.png)

• 回归分析——确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。

• 按照自变量的多少，分为一元回归和多元回归分析

• 按照因变量的多少，可分为简单回归分析和多重回归分析

• 按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析。

> 例子
>
> ![image-20241126142212689](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126142212689.png)

## 衡量拟合程度（最小二乘法）

![image-20241202085852778](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241202085852778.png)

![image-20241126155348616](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126155348616.png)

## 最小二乘法的几何意义

![image-20241126155831948](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126155831948.png)

![image-20241126160140171](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126160140171.png)

目标是通过调整 w，找到一个在  x行空间内的向量，使其与 Y 的距离最短。这就是目标函数的几何意义。

## 岭回归

![image-20241126172534609](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126172534609.png)

![image-20241126172519740](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241126172519740.png)

### 岭回归的惩罚机制

![image-20241202091304109](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241202091304109.png)

### 应用举例

![image-20241202092333555](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241202092333555.png)

### 优缺点

优点：

- 引入惩罚机制，惩罚大回归系数，解决多重线性问题
- 具有一定参数收缩的作用

缺点：

- 是通过岭迹图来判断数据，可能导致回归结果不准确，效果不佳
- **没有稀疏化选择变量的能力**

## LASSO回归

> 最主要的特点就是让某些回归系数趋于零

通过对回归系数的绝对值求和进行惩罚，**使得某些回归系数趋于零**，从而进行特征选择。

![image-20241202092849175](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241202092849175.png)

## 评价回归模型的指标

- 平均绝对误差

- 均方误差

- ![image-20241202093113068](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241202093113068.png)

  ## 拟合优度

  ![image-20241202093309617](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241202093309617.png)

# 逻辑回归

逻辑回归的本质是：假设数据服从伯努利分布，**使用极大似然估计法进行参**

数估计

![image-20241127111238693](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241127111238693.png)

### 逻辑回归的解决思路

逻辑回归的思路是，**先拟合决策边界(不局限于线性，还可以是多项式)**，再建

立这个**边界与分类的概率联系**，从而得到了二分类情况下的概率。

# K-近邻



![image-20241128110631110](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241128110631110.png)

支持向量机里面，支持向量代表距离超平面最近



![image-20241128135537186](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241128135537186.png)

### K值选择总结

k值过大 欠拟合---------只有与测试样本比较近的会起作用

过小：过拟合

选择后可以使用交叉验证法来选取最优的k值

![image-20241128113203552](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241128113203552.png)

### 优缺点

![image-20241128135904285](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241128135904285.png)

### 距离的度量

曼哈顿距离就是直接减



![image-20241202102049856](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241202102049856.png)

![image-20241202102054831](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241202102054831.png)

# 指标评价

![image-20241128140122976](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241128140122976.png)

![image-20241128140322589](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241128140322589.png)

下面这个感觉把不平衡分类的指标记住就好了，**然后把指标的计算公式算出来**

![image-20241128140410441](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241128140410441.png)

# 朴素贝叶斯的优缺点

![image-20241128141333697](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241128141333697.png)

# 决策树

![image-20241128141551286](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241128141551286.png)

![image-20241128141603186](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241128141603186.png)

**步骤**

- 特征选择

  ![image-20241129170641570](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129170641570.png)

## 信息量

![image-20241129170935325](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129170935325.png)

### 信息量的计算！

这个感觉很有可能考

![image-20241129170945889](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129170945889.png)

## 信息熵

信源所含有的信息量称为**信息熵**是指每

是指个符号所含信息量的**统计平均值**。m种符号的平均信息量（熵）为：

![image-20241129171059924](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129171059924.png)

计算解释

![image-20241129171401807](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129171401807.png)

## 信息量

信息**熵越大**，**纯度越小**

![image-20241129171604734](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129171604734.png)

计算的时候是求和每个信息熵然后取负数，**记住这个公式应该差不多**

![image-20241129171819564](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129171819564.png)

对总样本进行分类

![image-20241129171919168](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129171919168.png)

![image-20241129171955739](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129171955739.png)

## 信息增益

![image-20241129172340234](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129172340234.png)

计算解释

![image-20241129172546010](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129172546010.png)

![image-20241129172618159](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129172618159.png)

## C4.5算法 信息增益率

**C4.5算法中采用Gain ratio代替Information gain**，引入增益率



![image-20241129172720083](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241129172720083.png)

![image-20241201163606207](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201163606207.png)

![image-20241201163854203](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201163854203.png)

计算例子

计算概率的分母换成全部样本的，这里全部样本是14

如果是算信息量就是income的样本

![image-20241201165247131](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201165247131.png)

## Gimi指数

![image-20241201165426296](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201165426296.png)

![image-20241201170303046](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201170303046.png)

- 第二步生成决策树
- ![image-20241201170323383](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201170323383.png)

第三步

![image-20241201170425796](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201170425796.png)

## 预剪枝

优缺点

- 缺点 ： 难以精准预测生长周期

  ​				可能存在过拟合，过早停止

![image-20241201171544123](C:%5CUsers%5C%E8%B0%A2%E9%9A%86%E6%9D%B0%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241201171544123.png)

后剪枝

![image-20241201171702480](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201171702480.png)

## 随机森林

![image-20241201171933906](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201171933906.png)

![image-20241201180708974](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201180708974.png)

**通常称由单个学习算法（如决策树算法、神经网络算法等）训练数据**

**得到的学习器称为弱分类器或基分类器。**

## 集成学习如何构造

![image-20241201181558910](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201181558910.png)

> 如何构造？

- Boosting：称为**自助法**，是一种有放回的抽样
- Bagging：

![image-20241201182442223](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201182442223.png)



![image-20241201182558287](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201182558287.png)

## 随机森林中的数量影响

![image-20241201182721056](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201182721056.png)

与svm的区别

![image-20241201182909936](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201182909936.png)

## 随机森林的特点

- 调整参数少，不容易过拟合，能高效处理大样本
- 能同时调整样本和特征子集来获得不同的弱分类器

![image-20241201183032011](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201183032011.png)





# 无监督学习

## 聚类

没有标签

**所以通过样本的相似性来评估样本所属**

![image-20241201190854283](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201190854283.png)

**簇是通过相似性来划分的**，应用看一下就好了

![image-20241201191013693](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201191013693.png)

### 好的聚类的性质

![image-20241201191431823](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201191431823.png)

## K-means（基于划分）

通过k-means开划分聚类

**聚类是要类间距离远，类内距离近，这样的划分是好的划分**

![image-20241201191723299](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201191723299.png)

![image-20241201191841794](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201191841794.png)

### 直观理解

![image-20241201191942258](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201191942258.png)

### 计算过程

总结：先取12做初始，然后不断跟新距离，知道不再变化就是最后的中心

![image-20241201192152989](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201192152989.png)

![image-20241201192250747](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201192250747.png)



![image-20241201192402234](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201192402234.png)



### 优缺点

总结：

优点

- 时间复杂度低，速度快

**缺点**

- 需要单独制定k的划分个数
- 容易陷入局部最优
- 对异常值敏感，遇到的话簇的中心可能会偏移

![image-20241201192449786](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201192449786.png)

### 如何找到k的数量

找到拐点

![image-20241201192727770](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201192727770.png)



## K-means++

### 与k-means的区别

就是不是简单的按照从前往后的顺序进行获得初始分类，**而是在拿了一定的时候，拿远的换份**

![image-20241201193200089](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201193200089.png)

## K-medoids

![image-20241201193837992](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201193837992.png)



**选用簇中位置最中心的对象，即中心点**

![image-20241201194149855](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201194149855.png)





![image-20241201195438665](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201195438665.png)





![image-20241201195807944](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201195807944.png)

## DBSCAN

**更加健壮，对异常值具有鲁棒性**

**可以发现任意形状的聚类簇**

![image-20241201195834030](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201195834030.png)

### 需要的两个参数

- 领域半径
- 最少点数目

![image-20241201195958677](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201195958677.png)



### 可达性和连通性

![image-20241201200158698](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201200158698.png)

​	**可达性说明一个数据点是否可以直接或间接地从另一个数据点访问，而连接性**

**说明两个数据点是否属于同一个集群。**

![image-20241201200204575](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201200204575.png)





#### 可达性与连通性的三个划分

![image-20241201200308260](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201200308260.png)

### 优缺点

优点：

- 不需要事先划分
- 对异常值不敏感

缺点

- 对于密度区分不明显的样本聚类效果差
- 样本较大的情况收敛时间较长

![image-20241201200333257](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201200333257.png)

## 聚类的评价指标

- 正确率
- 标准互信息量

![image-20241201200510538](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201200510538.png)



![image-20241201200521385](https://raw.githubusercontent.com/xiechen274/ChenCsNote/images/images/image-20241201200521385.png)



























